{
    "configuration": {
        "arch_config": "/scratch/user/sabuj.laskar/SuperMesh_2/src/SCALE-Sim/configs/express.cfg",
        "num_hmcs": 16,
        "num_vaults": 16,
        "mini_batch_size": 256,
        "network": "/scratch/user/sabuj.laskar/SuperMesh_2/src/SCALE-Sim/topologies/mlperf/AlphaGoZero.csv",
        "run_name": "AlphaGoZero",
        "outdir": "/scratch/user/sabuj.laskar/SuperMesh_2/results/micro2025/compute_short_32_b256",
        "booksim_network": "mesh",
        "booksim_config": "/scratch/user/sabuj.laskar/SuperMesh_2/src/booksim2/runfiles/mesh/anynet_mesh_16_200.cfg",
        "allreduce": "multitree",
        "verbose": false,
        "collective": "Compute",
        "message_buffer_size": 32,
        "message_size": 4096,
        "synthetic_data_size": 0,
        "bandwidth": 100,
        "load_tree": true,
        "only_save_tree": false,
        "messages_per_chunk": 3,
        "save_link_utilization": false,
        "analytical": false,
        "per_dim_nodes": 4,
        "flits_per_packet": 17,
        "radix": 4,
        "latency": 20,
        "per_message_time": 360,
        "extension": "Compute",
        "pe_array_height": 32,
        "pe_array_width": 32,
        "ifmap_sram_size": 1048576,
        "filter_sram_size": 1048576,
        "ofmap_sram_size": 1048576,
        "ifmap_offset": 0,
        "filter_offset": 10000000,
        "ofmap_offset": 20000000,
        "ifmap_grad_offset": 40000000,
        "filter_grad_offset": 50000000,
        "ofmap_grad_offset": 30000000,
        "data_flow": "os",
        "multitree_total_message": 97,
        "nodes": 16,
        "saved_tree_name": "/scratch/user/sabuj.laskar/SuperMesh_2/src/SavedTrees/mesh/mesh_multitree_16",
        "dump": false
    },
    "results": {
        "performance": {
            "training": 1251840,
            "training_by_layer": {
                "7": 416029,
                "6": 421900,
                "5": 422227,
                "4": 428227,
                "3": 434098,
                "2": 821808,
                "1": 1209518,
                "0": 1248766
            },
            "total": 1251841,
            "allreduce": {
                "total": 1
            }
        },
        "power": {
            "network": {
                "dynamic": NaN,
                "static": 9.36868793981943,
                "total": NaN,
                "router": {
                    "dynamic": NaN,
                    "static": 9.27930968087543,
                    "total": NaN
                },
                "link": {
                    "dynamic": NaN,
                    "static": 0.08937825894400027,
                    "total": NaN,
                    "flits": 0
                }
            }
        }
    }
}